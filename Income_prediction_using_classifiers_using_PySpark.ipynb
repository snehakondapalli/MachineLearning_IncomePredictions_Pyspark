{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/snehakondapalli/MachineLearning_IncomePredictions_Pyspark/blob/main/Income_prediction_using_classifiers_using_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7f410824-1dcc-4435-8c1f-d3e51cdfb579",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_sIeV3hbz4X",
        "outputId": "2e093d10-f408-4457-85f5-852d2be4f7c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.0.3)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=c4fdd2bd9c11f97680ec5456337259dd9d0f558d20f8e82537db8bd62149a825\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ucimlrepo\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0a3aca6b-70f5-4702-9c7f-cb7fe476968b",
          "showTitle": false,
          "title": ""
        },
        "id": "IxhFhfeDbz4Y"
      },
      "source": [
        "\n",
        "## Data collection\n",
        "\n",
        "### Load the “adult dataset” from https://archive.ics.uci.edu/ml/datasets/adult."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "dd194da9-03b1-4f33-b6cc-c6f9ef0a92c4",
          "showTitle": false,
          "title": ""
        },
        "id": "LdomoSbSbz4Y"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import isnan, when, count, col, lit\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Normalizer\n",
        "from pyspark.sql import Window\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "adult = fetch_ucirepo(id=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "780ecab0-0bd0-49dc-b645-e73c577ffef2",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhIh1SKfbz4Y",
        "outputId": "729634d2-0001-47e1-9681-a7722280c586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+------+------------+-------------+------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+----+------+\n",
            "|age|       workclass|fnlwgt|   education|education-num|    marital-status|       occupation| relationship|              race|   sex|capital-gain|capital-loss|hours-per-week|native-country|  id|income|\n",
            "+---+----------------+------+------------+-------------+------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+----+------+\n",
            "| 19|         Private|168294|     HS-grad|            9|     Never-married|     Craft-repair|    Own-child|             White|  Male|           0|           0|            40| United-States|  26| <=50K|\n",
            "| 49|         Private|193366|     HS-grad|            9|Married-civ-spouse|     Craft-repair|      Husband|             White|  Male|           0|           0|            40| United-States|  29| <=50K|\n",
            "| 67|         Private| 49401|   Assoc-voc|           11|          Divorced|    Other-service|Not-in-family|             White|Female|           0|           0|            24| United-States| 474| <=50K|\n",
            "| 28|         Private| 57066|     HS-grad|            9|Married-civ-spouse| Transport-moving|      Husband|             White|  Male|           0|           0|            40| United-States| 964| <=50K|\n",
            "| 24|         Private| 38455|Some-college|           10|     Never-married|     Craft-repair|Not-in-family|             White|  Male|           0|           0|            40| United-States|1677| <=50K|\n",
            "| 35|         Private|123606|Some-college|           10|Married-civ-spouse|Handlers-cleaners|      Husband|             White|  Male|           0|           0|            40| United-States|1697| <=50K|\n",
            "| 59|         Private|183606|        11th|            7|Married-civ-spouse|Machine-op-inspct|      Husband|             White|  Male|           0|           0|            40| United-States|1806| <=50K|\n",
            "| 44|Self-emp-not-inc|483201|   Bachelors|           13|Married-civ-spouse|  Exec-managerial|      Husband|             White|  Male|           0|           0|            40| United-States|1950| <=50K|\n",
            "| 23|         Private|194102|   Bachelors|           13|     Never-married|  Exec-managerial|    Unmarried|             White|  Male|           0|           0|            40| United-States|2040| <=50K|\n",
            "| 64|Self-emp-not-inc|177825|     HS-grad|            9|     Never-married|    Other-service|Not-in-family|             White|  Male|        1055|           0|            40| United-States|2214| <=50K|\n",
            "| 39|         Private| 34028|   Bachelors|           13|          Divorced|  Exec-managerial|    Unmarried|             White|Female|           0|           0|            48| United-States|2250| <=50K|\n",
            "| 24|Self-emp-not-inc|174391|     HS-grad|            9|Married-civ-spouse|     Adm-clerical|         Wife|             White|Female|           0|           0|            40| United-States|2453| <=50K|\n",
            "| 31|         Private|133770|  Assoc-acdm|           12|Married-civ-spouse|     Craft-repair|Not-in-family|Asian-Pac-Islander|  Male|           0|           0|            50| United-States|2509|  >50K|\n",
            "| 49|     Federal-gov|213668|     Masters|           14|     Never-married|  Exec-managerial|Not-in-family|             White|  Male|           0|           0|            56| United-States|2529|  >50K|\n",
            "| 52|         Private| 91048|     HS-grad|            9|          Divorced|Machine-op-inspct|    Own-child|             Black|Female|           0|           0|            35| United-States|2927| <=50K|\n",
            "| 48|         Private|331482| Prof-school|           15|Married-civ-spouse|     Tech-support|      Husband|             White|  Male|           0|        1977|            40| United-States|3091|  >50K|\n",
            "| 23|Self-emp-not-inc|208503|Some-college|           10|          Divorced|     Adm-clerical|    Own-child|             White|  Male|           0|           0|            40| United-States|3506| <=50K|\n",
            "| 19|         Private|202673|Some-college|           10|     Never-married|Handlers-cleaners|    Own-child|             White|  Male|           0|           0|            50| United-States|3764| <=50K|\n",
            "| 26|         Private|164018|   Bachelors|           13|Married-civ-spouse|            Sales|      Husband|             White|  Male|        4064|           0|            50| United-States|4590| <=50K|\n",
            "| 34|         Private|424988|     HS-grad|            9|Married-civ-spouse|     Craft-repair|      Husband|             White|  Male|           0|           0|            45| United-States|4823| <=50K|\n",
            "+---+----------------+------+------------+-------------+------------------+-----------------+-------------+------------------+------+------------+------------+--------------+--------------+----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "X_features = adult.data.features\n",
        "y_target = adult.data.targets\n",
        "\n",
        "X_features = spark.createDataFrame(X_features)\n",
        "Y = spark.createDataFrame(y_target)\n",
        "\n",
        "X_features = X_features.withColumn(\"id\", monotonically_increasing_id())\n",
        "Y = Y.withColumn(\"id\", monotonically_increasing_id())\n",
        "\n",
        "\n",
        "X = X_features.join(Y, X_features.id == Y.id, 'inner').drop(Y.id)\n",
        "\n",
        "X.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b01d3112-6a6d-4641-b869-5df7fde93ab6",
          "showTitle": false,
          "title": ""
        },
        "id": "9qe7Pw9Kbz4Z"
      },
      "source": [
        "\n",
        "## Data cleaning .\n",
        "\n",
        "### Handle the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4d9c0420-1f14-4357-b87b-645e1eecce96",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLCO1LvVbz4Z",
        "outputId": "87621f3a-2fb9-4e77-b0b8-0ebff5c0bb1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+------+---------+-------------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+---+------+\n",
            "|age|workclass|fnlwgt|education|education-num|marital-status|occupation|relationship|race|sex|capital-gain|capital-loss|hours-per-week|native-country| id|income|\n",
            "+---+---------+------+---------+-------------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+---+------+\n",
            "|  0|      963|     0|        0|            0|             0|       966|           0|   0|  0|           0|           0|             0|           274|  0|     0|\n",
            "+---+---------+------+---------+-------------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check for any nan or null values in the Dataframe\n",
        "X.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in X.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "aa409ce6-bb66-424c-8288-f05ae5dbb58e",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5E78MQubz4Z",
        "outputId": "70095e89-ca41-41a5-cd94-45f2f255e2b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+------+---------+-------------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+---+------+\n",
            "|age|workclass|fnlwgt|education|education-num|marital-status|occupation|relationship|race|sex|capital-gain|capital-loss|hours-per-week|native-country| id|income|\n",
            "+---+---------+------+---------+-------------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+---+------+\n",
            "|  0|      963|     0|        0|            0|             0|       966|           0|   0|  0|           0|           0|             0|           274|  0|     0|\n",
            "+---+---------+------+---------+-------------+--------------+----------+------------+----+---+------------+------------+--------------+--------------+---+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Replace \"?\" with unknown\n",
        "X = X.select([when(col(c)==\"?\",\"Unknown\").otherwise(col(c)).alias(c) for c in X.columns])\n",
        "\n",
        "# Replace null values with unknown\n",
        "X = X.select([when(col(c).isNull(),\"Unknown\").otherwise(col(c)).alias(c) for c in X.columns])\n",
        "\n",
        "# Convert <=50K. and >50K. to just <=50K and >50K\n",
        "X = X.select([when(col(c) == '<=50K.',\"<=50K\").otherwise(col(c)).alias(c) for c in X.columns])\n",
        "X = X.select([when(col(c) == '>50K.',\">50K\").otherwise(col(c)).alias(c) for c in X.columns])\n",
        "\n",
        "# Check if there are any null values remaining\n",
        "X.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in X.columns]).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "e1ea079d-7f57-470d-ab52-bab29b89b48f",
          "showTitle": false,
          "title": ""
        },
        "id": "Nl8SgsFCbz4Z"
      },
      "source": [
        "\n",
        "## Feature engineering.\n",
        "\n",
        "### Distill the features (all columns ex-cept for income) and labels (income). Transform the features into vectors. Use one-hot encoder to process categorical features. Split the dataset into a training and testing set with a ratio of 80% v.s. 20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ee9e783f-28b8-4254-ae7b-3ee5294129a5",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wlHTaoMbz4Z",
        "outputId": "28129880-1f92-4e3f-f42a-5b4b93e66b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|label|\n",
            "+-----+\n",
            "|  0.0|\n",
            "|  1.0|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indexed_X = 0;\n",
        "i = 0\n",
        "for column_name in X.columns:\n",
        "    # String Indexer Initialization\n",
        "    indexer = StringIndexer(inputCol=column_name, outputCol=f\"{column_name}_indexed\")\n",
        "    indexerModel = indexer.fit(X if i == 0 else indexed_X)\n",
        "\n",
        "    # Transform the DataFrame using the fitted StringIndexer model\n",
        "    indexed_X = indexerModel.transform(X if i == 0 else indexed_X)\n",
        "    i = i + 1\n",
        "\n",
        "indexed_X = indexed_X.drop(*X.columns)\n",
        "\n",
        "input_columns = [a for a in indexed_X.columns if a!='income_indexed']\n",
        "encoder = OneHotEncoder(inputCols=input_columns, outputCols=[a for a in X.columns if a!='income'])\n",
        "model = encoder.fit(indexed_X)\n",
        "encoded = model.transform(indexed_X)\n",
        "encoded = encoded.drop(*input_columns)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=X_features.columns, outputCol='features_assembled')\n",
        "\n",
        "features_vectorized = assembler.transform(encoded)\n",
        "\n",
        "normalizer = Normalizer(inputCol=\"features_assembled\", outputCol=\"features\", p=1.0)\n",
        "l1NormData = normalizer.transform(features_vectorized)\n",
        "l1NormData = l1NormData.withColumnRenamed('income_indexed', 'label')\n",
        "\n",
        "train, test = l1NormData.randomSplit(weights=[0.8,0.2], seed=200)\n",
        "\n",
        "# train.display()\n",
        "\n",
        "train.select('label').distinct().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "54cb9dc3-a671-48e9-8b1e-7242497a2824",
          "showTitle": false,
          "title": ""
        },
        "id": "wmv61AN3bz4Z"
      },
      "source": [
        "\n",
        "## Training.\n",
        "\n",
        "### Build a logistic regression and a gradient-boostedtree model to fit the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7407defb-77e0-44b7-88a7-b85f1c619b2b",
          "showTitle": false,
          "title": ""
        },
        "id": "u5Pr2LqLbz4Z"
      },
      "outputs": [],
      "source": [
        "# Logistic regression Model\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
        "\n",
        "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=2, regParam=0.001)\n",
        "\n",
        "lr_model = lr.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "0380f80b-2658-48e9-a638-23906e365219",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrdrENsfbz4Z",
        "outputId": "56845a6e-e344-4049-836b-8720aa1f33f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline_e6212bbfadd9"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=2)\n",
        "\n",
        "pipeline = Pipeline(stages=[gbt])\n",
        "\n",
        "gbt_model = pipeline.fit(train)\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ce0f6613-1bb6-4131-931b-d08a96d32fa6",
          "showTitle": false,
          "title": ""
        },
        "id": "LoWZctcMbz4Z"
      },
      "source": [
        "\n",
        "## Tuning and Evaluation.\n",
        "\n",
        "### For each model, use grid parameter search and cross validation over 5 folds to find the parameters that yields the highest areaUnderROC on the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b9d0b77c-b6eb-4930-afca-9c4d5207b405",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4VUHNHtbz4Z",
        "outputId": "dc6a1276-a378-4beb-c9bf-18913989b446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Iterations ->  1\n",
            "Objective History ->  [0.550250619060374, 0.18879399589650184]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9938453584160494"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Importing the evaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1]).build()\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "\n",
        "cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator , parallelism=2, numFolds=5)\n",
        "\n",
        "train_data = l1NormData.drop(*X_features.columns)\n",
        "train_data = train_data.drop('features_assembled')\n",
        "\n",
        "cvModel = cv.fit(train_data)\n",
        "\n",
        "trainingSummary = cvModel.bestModel.summary\n",
        "\n",
        "print(\"Total Iterations -> \", trainingSummary.totalIterations)\n",
        "print(\"Objective History -> \", trainingSummary.objectiveHistory)\n",
        "\n",
        "evaluator.evaluate(cvModel.transform(train_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "fc64bd3f-9b95-4426-a2e0-d04556879299",
          "showTitle": false,
          "title": ""
        },
        "id": "ewnOvV8ebz4Z"
      },
      "source": [
        "\n",
        "## Prediction.\n",
        "\n",
        "### For each model, make predictions on the testing set and display the areaUnderROC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5b976d5c-653e-4916-91bf-13b123f4f4e7",
          "showTitle": false,
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIgPR58wbz4Z",
        "outputId": "d925ace5-cb4d-4959-f05a-d3203eea4442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC on Logistic Regression Model :  0.7781033904535466\n",
            "AUC on Logistic Regression Model :  0.7391428977571507\n"
          ]
        }
      ],
      "source": [
        "\n",
        "lr_prediction = lr_model.transform(test)\n",
        "\n",
        "# Calling the evaluator\n",
        "res = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='label')\n",
        "\n",
        "# Evaluating the AUC on Logistic Regression Model\n",
        "ROC_AUC_LR = res.evaluate(lr_prediction)\n",
        "\n",
        "print(\"AUC on Logistic Regression Model : \", ROC_AUC_LR)\n",
        "\n",
        "gbt_prediction = gbt_model.transform(test)\n",
        "\n",
        "# Evaluating the AUC on Logistic Regression Model\n",
        "ROC_AUC_GBT = res.evaluate(gbt_prediction)\n",
        "\n",
        "print(\"AUC on Logistic Regression Model : \", ROC_AUC_GBT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bd7d325a-29af-41cd-83ed-358538a9c41c",
          "showTitle": false,
          "title": ""
        },
        "id": "GgAA6yFrbz4a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "Big_data_Final_project_SnehaKondapalli",
      "widgets": {}
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}